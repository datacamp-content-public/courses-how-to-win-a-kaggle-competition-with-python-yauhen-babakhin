---
title: Insert title here
key: 38086466ef7244576a066cefcf1affa2

---
## Baseline Model

```yaml
type: "TitleSlide"
key: "4b84766eb0"
```

`@lower_third`

name: Yauhen Babakhin
title: Data Scientist, Profitero


`@script`
After you’ve learnt about the initial steps in the competition and local validation, let’s take a look at the modeling stage.


---
## Modeling Stage

```yaml
type: "FullSlide"
key: "4f4a20c12f"
```

`@part1`
![datacamp1.png](https://assets.datacamp.com/production/repositories/4329/datasets/48424bc5f80d5feee925f4e1a04a22168ae16518/datacamp1.png){{2}}


`@script`
This stage is the longest one in the competition, and reminds some kind of a marathon: in the loop you create new features, enhance your models, apply different tricks over and over again.
The majority of the ideas and experiments will not work, but the goal is to find a subsample of tricks which improves both your local validation score and Public Leaderboard.


---
## Modeling Stage

```yaml
type: "FullSlide"
key: "7df2a95bf8"
disable_transition: true
center_content: true
```

`@part1`
![datacamp_2.png](https://assets.datacamp.com/production/repositories/4329/datasets/2472a62b6a648eb44e2200caf6087c78d9420abd/datacamp_2.png)


`@script`
To start with, you should establish the baseline model. It’s usually a very simple model that allows to check the whole pipeline you’ve wrote: review the local validation process and generate the first submissions for the test data.


---
## Titanic Data Preparation

```yaml
type: "FullCodeSlide"
key: "e646947d83"
center_content: false
disable_transition: false
```

`@part1`
```python
import pandas as pd
from sklearn.model_selection import train_test_split

train = pd.read_csv('../input/train.csv')
test = pd.read_csv('../input/test.csv')

validation_train, validation_test = train_test_split(train,
                                    test_size=0.3,
                                    random_state=123)
```{{2}}


`@script`
Let’s again work with Titanic Competition Dataset. We need to predict whether a passenger survived or not in the accident.

The Leaderboard metric is an accuracy over 2 classes. We will use the 30% holdout sample as a local validation.

So, let's read the train and test data and create simple holdout split using train_test_split function from scikit-learn.


---
## Baseline Model I

```yaml
type: "FullSlide"
key: "6ce967b834"
center_content: true
```

`@part1`
```python
naive_prediction = 1 if np.mean(train.Survived) > 0.5 else 0
test['Survived'] = naive_prediction
test[['PassengerId','Survived']].to_csv('mean_sub.csv', index=False)
```

&nbsp;

| Validation Accuracy  | Public LB Accuracy |
| ------------- | ------------- |
| 0.6343 | 0.6268 |{{2}}

![naive_1.png](https://assets.datacamp.com/production/repositories/4329/datasets/eeae640b9ca56ac4526d565e073a35c16d1ebefa/naive_1.png){{2}}


`@script`
The simplest model is to assign the most frequent class to all the test observations.
For this purpose, take the mean of Survived column over the whole train set and assign 1 if it is greater than 0.5, and assign 0 otherwise.
One can see that this mean is equal to 0.3 for the whole train set. So, we just assign zeroes to all the observations in the test set.

Such approach gives about 0.63 accuracy in both Local Validation and Public LB.


---
## Baseline Model II

```yaml
type: "FullSlide"
key: "1df95c70b9"
center_content: true
```

`@part1`
```python
naive_prediction_groups = train.groupby(['Sex','Pclass']) \
                               .Survived \
                               .aggregate({'Survived':'mean'}) \
                               .reset_index()
test = test.merge(naive_prediction_groups)
test['Survived'] = np.where(test['Survived'] > 0.5, 1, 0)

test[['PassengerId','Survived']].to_csv('mean_group_sub.csv', index=False)
```

&nbsp;

| Validation Accuracy  | Public LB Accuracy |
| ------------- | ------------- |
| 0.7985 | 0.7560 |{{2}}


![naive_2.png](https://assets.datacamp.com/production/repositories/4329/datasets/88df24307e2c02169fd6e3133a12dc473abf71cc/naive_2.png){{2}}


`@script`
A bit more complicated model is a mean grouped by the Sex and Cabin Class. The idea is the same: assign 1 to the whole group if the mean of the Survived columns is over 0.5 there. Otherwise, assign 0 to the whole group.

Such model achieves 0.80 Accuracy in Local validation and Public Leaderboard score is 0.76.


---
## Baseline Model III

```yaml
type: "FullSlide"
key: "77f482bf0a"
center_content: true
```

`@part1`
```python
from sklearn.linear_model import LogisticRegression
train['Sex'] = train['Sex'].map( {'female': 1, 'male': 0} )
test['Sex'] = test['Sex'].map( {'female': 1, 'male': 0} )

features = ['Pclass','Sex','SibSp','Parch']
clf = LogisticRegression(random_state=0).fit(train[features], train.Survived)
test['Survived'] = clf.predict(test[features])
test[['PassengerId','Survived']].to_csv('log_reg_sub.csv', index=False)
```
&nbsp;

| Validation Accuracy  | Public LB Accuracy |
| ------------- | ------------- |
| 0.8134 | 0.7703 |{{2}}

![naive_3.png](https://assets.datacamp.com/production/repositories/4329/datasets/f79756c168a15f059fa1512afacf16611f5bede0/naive_3.png){{2}}


`@script`
Finally, we could apply a simple Logistic Regression model on a small subset of features. Discard all the features which need additional preprocessing and just map Sex into binary variable.

All this data is fed into Logistic Regression model and here are the results: 0.81 Accuracy in Local Validation and 0.77 on the Public Leaderboard.


---
## Intermediate Results

```yaml
type: "FullSlide"
key: "8f7589cf24"
center_content: true
```

`@part1`
| Model | Validation Accuracy  | Public LB Accuracy |
| ------------- | ------------- | ------------- |
| Simple Mean | 0.6343 | 0.6268 |
| Group Mean | 0.7985 | 0.7560 |
| Logistic Regression | 0.8134 | 0.7703 |


`@script`
Now you have at least three simple submissions with local and public scores. Let's take a look at them.

One can easily see that local score correlates with the Public (the correlation is not perfect, but the higher local score, the higher it is on the Public LB). It is a good sign and we could proceed with such a naive validation scheme.


---
## Let's Practice!

```yaml
type: "FinalSlide"
key: "3c9057ef23"
```

`@script`
In the next lessons, we'll explore the improvements of these models: using feature engineering, various model types and parameter tuning.
Now, let's move on to practice. You will create another couple of baseline models: one based on grouping and another one based on Random Forest.

